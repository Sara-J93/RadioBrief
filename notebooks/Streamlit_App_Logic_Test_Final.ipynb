{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxlf33TSPtab",
        "outputId": "a68d787d-519d-4880-b564-fbfcbd0f437d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries for logic testing and Streamlit...\n",
            "\\nForce installing compatible httpx version (0.27.2)...\n",
            "Found existing installation: httpx 0.27.2\n",
            "Uninstalling httpx-0.27.2:\n",
            "  Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\\n--- Installation Complete ---\n",
            " Libraries installed.\n",
            " IMPORTANT: Please restart the runtime now before running the next cell!\n",
            "   Go to 'Runtime' -> 'Restart Runtime' in the menu above.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Installing Libraries\n",
        "print(\"Installing necessary libraries for logic testing and Streamlit...\")\n",
        "\n",
        "# Installing libraries needed for processing and potentially Streamlit later\n",
        "# Using -q to make the output less noisy\n",
        "!pip install streamlit pandas openai \"transformers[torch]\" torch pdfplumber python-dotenv nest_asyncio evaluate accelerate sentencepiece -q\n",
        "\n",
        "# Installing specific httpx version compatible with openai v1.10.0\n",
        "print(\"\\\\nForce installing compatible httpx version (0.27.2)...\")\n",
        "# Uninstalling any existing version first to avoid conflicts\n",
        "!pip uninstall -y httpx\n",
        "# Installing the required version\n",
        "!pip install httpx==0.27.2 --quiet\n",
        "\n",
        "print(\"\\\\n--- Installation Complete ---\")\n",
        "print(\" Libraries installed.\")\n",
        "print(\" IMPORTANT: Please restart the runtime now before running the next cell!\")\n",
        "print(\"   Go to 'Runtime' -> 'Restart Runtime' in the menu above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first step is like getting all the tools and ingredients ready before cooking. We need to install several Python packages (libraries) that provide specific functionalities like reading PDFs (pdfplumber), handling AI models (transformers, torch), interacting with OpenAI (openai), and running asynchronous code smoothly in Colab (nest_asyncio). We also install streamlit itself, even though we won't run the web app here, just to ensure all dependencies are present. We also install a specific older version of httpx because the openai library we use needs it."
      ],
      "metadata": {
        "id": "JHou6ZQ_Q3bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Drive, Load API Key Directly, and Init OpenAI Client\n",
        "\n",
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import os\n",
        "# from dotenv import load_dotenv # No longer needed for direct reading\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import traceback\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Apply nest_asyncio patch ---\n",
        "try:\n",
        "    nest_asyncio.apply()\n",
        "    print(\"Applied nest_asyncio patch.\")\n",
        "except RuntimeError:\n",
        "    print(\"nest_asyncio patch might already be applied or is not needed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not apply nest_asyncio: {e}\")\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"----- Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"----- Failed to mount Google Drive: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Load OpenAI API Key DIRECTLY from openai_key.txt ---\n",
        "# ** Define the CORRECT path to your key file **\n",
        "api_key_file_path = \"/content/drive/MyDrive/RadioBrief/openai_key.txt\"\n",
        "\n",
        "openai_api_key = None # Initialize variable\n",
        "print(f\"\\nAttempting to load API key directly from: {api_key_file_path}\")\n",
        "if os.path.exists(api_key_file_path):\n",
        "    try:\n",
        "        # Open the file and read the key (assuming it's the only content)\n",
        "        with open(api_key_file_path, \"r\") as f:\n",
        "            openai_api_key = f.read().strip() # Read the whole file and remove whitespace\n",
        "\n",
        "        if openai_api_key:\n",
        "            print(f\"----- OpenAI API key loaded successfully from {os.path.basename(api_key_file_path)}.\")\n",
        "        else:\n",
        "            print(f\"----- Found file {os.path.basename(api_key_file_path)}, but it appears empty.\")\n",
        "            openai_api_key = None # Ensure it's None if empty\n",
        "    except Exception as e:\n",
        "         print(f\"----- Error reading API key file {os.path.basename(api_key_file_path)}: {e}\")\n",
        "         openai_api_key = None\n",
        "else:\n",
        "     print(f\"----- Error: API key file not found at the specified path: {api_key_file_path}\")\n",
        "     print(\"   Please ensure the file exists in your Drive and the path is correct.\")\n",
        "\n",
        "# --- Initialize OpenAI Client ---\n",
        "async_client = None # Initialize variable\n",
        "if openai_api_key:\n",
        "    print(\"\\nInitializing OpenAI Async Client...\")\n",
        "    try:\n",
        "        # Create the client instance using the key we loaded\n",
        "        async_client = AsyncOpenAI(api_key=openai_api_key)\n",
        "        print(\"----- OpenAI Async Client Initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"----- Failed to initialize OpenAI client: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "else:\n",
        "    print(\"\\n----- Skipping OpenAI client initialization because the API key could not be loaded.\")\n",
        "\n",
        "print(\"\\n--- Setup Complete for Cell 2 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU6AmZMERltu",
        "outputId": "b9f4afb8-3c30-46fb-8b2a-50775a2ae123"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied nest_asyncio patch.\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted successfully.\n",
            "\n",
            "Attempting to load API key directly from: /content/drive/MyDrive/RadioBrief/openai_key.txt\n",
            "✅ OpenAI API key loaded successfully from openai_key.txt.\n",
            "\n",
            "Initializing OpenAI Async Client...\n",
            "✅ OpenAI Async Client Initialized successfully.\n",
            "\n",
            "--- Setup Complete for Cell 2 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Environment (Drive, Key, Async)\n",
        "\n",
        "Explanation: Now that the tools are installed (and the runtime restarted), we need to set up our workspace. This cell connects your Colab notebook to your Google Drive so we can access files (like your API key and saved model). It also loads your secret OpenAI API key from a .env file (which you should have in your Google Drive) so we don't put the key directly in the code. Finally, it prepares the environment to handle the special type of code (asynchronous) needed for OpenAI calls.\n",
        "\n",
        "##Changes Made:\n",
        "\n",
        "Removed the import load_dotenv.\n",
        "Changed env_path to api_key_file_path and set it to your correct file: /content/drive/MyDrive/RadioBrief/openai_key.txt.\n",
        "Replaced the load_dotenv(...) and os.getenv(...) logic with a simple with open(...) as f: f.read().strip() to read the key directly from the specified text file."
      ],
      "metadata": {
        "id": "9djFRJ1JRrFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load Fine-Tuned Classification Model\n",
        "\n",
        "# Import necessary libraries from transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the path where your fine-tuned model is saved in Google Drive\n",
        "# ** IMPORTANT: Double-check this path is correct! **\n",
        "fine_tuned_model_path = \"/content/drive/MyDrive/RadioBrief/finetune-results/final_model\"\n",
        "\n",
        "# Define the expected labels and mappings from your fine-tuning notebook\n",
        "# (These MUST match the labels the model was trained on - MasakhaNEWS 'fra')\n",
        "id2label_map = {0: 'business', 1: 'entertainment', 2: 'health', 3: 'politics', 4: 'religion', 5: 'sports', 6: 'technology'}\n",
        "label2id_map = {v: k for k, v in id2label_map.items()}\n",
        "num_model_labels = len(id2label_map)\n",
        "\n",
        "# --- Load Model ---\n",
        "classifier_finetuned = None # Initialize variable\n",
        "print(f\"Attempting to load fine-tuned classification model from: {fine_tuned_model_path}\")\n",
        "\n",
        "# Check if the specified directory exists\n",
        "if not os.path.isdir(fine_tuned_model_path):\n",
        "     print(f\"----- Error: Fine-tuned model directory not found at: {fine_tuned_model_path}\")\n",
        "     print(\"   Please check the path. Did you run the fine-tuning notebook and save the model?\")\n",
        "else:\n",
        "    try:\n",
        "        # Determine if GPU is available, otherwise use CPU\n",
        "        device_id = 0 if torch.cuda.is_available() else -1\n",
        "        device_name = 'GPU 0' if device_id == 0 else 'CPU'\n",
        "        print(f\"Attempting to load model onto device: {device_name}\")\n",
        "\n",
        "        # Load the model configuration and weights\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            fine_tuned_model_path,\n",
        "            num_labels=num_model_labels,\n",
        "            id2label=id2label_map,\n",
        "            label2id=label2id_map\n",
        "        )\n",
        "        # Load the tokenizer associated with the model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "        # Create a text-classification pipeline using the loaded model and tokenizer\n",
        "        classifier_finetuned = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device_id # Tell the pipeline to use GPU if available\n",
        "        )\n",
        "        print(f\"----- Fine-tuned classifier pipeline loaded successfully on {device_name}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during loading\n",
        "        print(f\"----- Failed to load fine-tuned classification model: {e}\")\n",
        "        print(traceback.format_exc()) # Show detailed error\n",
        "\n",
        "# Final check\n",
        "if not classifier_finetuned:\n",
        "    print(\"\\n----- Classifier pipeline could not be loaded. Classification steps later might fail or be skipped.\")\n",
        "\n",
        "print(\"\\n--- Setup Complete for Cell 3 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLSPGGvTRz8U",
        "outputId": "cf09f389-f343-4847-c7a2-22c0b17485d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load fine-tuned classification model from: /content/drive/MyDrive/RadioBrief/finetune-results/final_model\n",
            "Attempting to load model onto device: CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuned classifier pipeline loaded successfully on CPU.\n",
            "\n",
            "--- Setup Complete for Cell 3 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Fine-Tuned Model\n",
        "\n",
        "Explanation: This cell loads the classification model you trained in the FineTune_Topic_Classifier (2).ipynb notebook. It finds the saved model files in your Google Drive and prepares a pipeline object (like a ready-to-use tool) that we can give text to and get a topic prediction back. It uses the specific labels the model was trained on (business, politics, etc.)."
      ],
      "metadata": {
        "id": "hs8-7ybWR4hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define Helper Functions and Variables\n",
        "\n",
        "# Import necessary libraries (some might be redundant but safe)\n",
        "import re\n",
        "import pdfplumber\n",
        "import logging\n",
        "import traceback\n",
        "import datetime\n",
        "import asyncio\n",
        "from openai import AsyncOpenAI # For type hint\n",
        "from transformers import pipeline # For type hint\n",
        "import os # Needed for basename in extract_text_from_pdf\n",
        "\n",
        "print(\"Defining helper functions and variables...\")\n",
        "\n",
        "# --- Configuration Variables (from Perfect_results_Rafael_WithReport.ipynb) ---\n",
        "# === ACTION REQUIRED: Paste your 'political_keywords' list below ===\n",
        "# (Copied from Cell 6 of Perfect_results_Rafael_WithReport.ipynb)\n",
        "political_keywords = [\n",
        "    # --- General Politics & Concepts ---\n",
        "    \"Politique\", \"Géopolitique\", \"Économie politique\", \"Relations internationales\",\n",
        "    \"Souveraineté\", \"Frontières\", \"Sécurité\", \"Défense\", \"Libéralisme\",\n",
        "    \"Conflit\", \"Guerre\", \"Crise\", \"Instabilité\", \"Tensions géopolitiques\", \"Cessez-le-feu\", \"Trêve\",\n",
        "    \"Élections\", \"Scrutin\", \"Parité\",\n",
        "    \"Diplomatie\", \"Négociations\", \"Sommet\", \"Traité\", \"Accord\",\n",
        "    \"Sanctions\", \"Embargo\", \"Coercition économique\",\n",
        "    \"Aide humanitaire\", \"Crise humanitaire\", \"Droits de l'homme\",\n",
        "    \"Terrorisme\", \"Extrémisme\", \"Ingérence étrangère\",\n",
        "    # --- French Politics & Foreign Policy ---\n",
        "    \"Assemblée Nationale\", \"Sénat\", \"Matignon\", \"Bercy\", \"Élysée\",\n",
        "    \"Quai d'Orsay\", \"Ministère des Affaires étrangères\", \"Ministre des Affaires étrangères\", \"France\",\n",
        "    # --- International Institutions & Law ---\n",
        "    \"ONU\", \"Nations Unies\", \"Conseil de sécurité\", \"Résolution\", \"Casques bleus\",\n",
        "    \"OTAN\", \"NATO\",\n",
        "    \"Union Européenne\", \"UE\", \"Bruxelles\", \"Commission européenne\", \"Parlement européen\",\n",
        "    \"G7\", \"G20\", \"OMC\", \"FMI\", \"Banque mondiale\",\n",
        "    \"Union Africaine\", \"UA\",\n",
        "    \"Cour pénale internationale\", \"CPI\", \"Cour internationale de Justice\", \"CIJ\",\n",
        "    # --- Regions & Countries ---\n",
        "    \"Moyen-Orient\", \"Proche-Orient\",\n",
        "    \"Gaza\", \"Palestinien\", \"Palestine\", \"Cisjordanie\", \"Jérusalem\", \"Autorité palestinienne\",\n",
        "    \"Israël\", \"Israélien\", \"Tel Aviv\",\n",
        "    \"Liban\", \"Libanais\", \"Beyrouth\",\n",
        "    \"Syrie\", \"Syrien\", \"Damas\",\n",
        "    \"Jordanie\", \"Jordanien\", \"Amman\",\n",
        "    \"Égypte\", \"Égyptien\", \"Le Caire\",\n",
        "    \"Irak\", \"Irakien\", \"Bagdad\",\n",
        "    \"Iran\", \"Iranien\", \"Téhéran\",\n",
        "    \"Arabie Saoudite\", \"Saoudien\", \"Riyad\",\n",
        "    \"Yémen\", \"Yéménite\", \"Sanaa\",\n",
        "    \"Qatar\", \"Qatari\", \"Doha\",\n",
        "    \"Émirats arabes unis\", \"EAU\", \"Émirati\", \"Abou Dhabi\", \"Dubaï\",\n",
        "    \"Turquie\", \"Turc\", \"Ankara\",\n",
        "    \"Afrique du Nord\", \"Maghreb\",\n",
        "    \"Algérie\", \"Algérien\", \"Alger\",\n",
        "    \"Tunisie\", \"Tunisien\", \"Tunis\",\n",
        "    \"Maroc\", \"Marocain\", \"Rabat\",\n",
        "    \"Libye\", \"Libyen\", \"Tripoli\",\n",
        "    \"États-Unis\", \"USA\", \"Américain\", \"Washington\", \"Maison Blanche\", \"Pentagone\", \"Département d'État\",\n",
        "    \"Chine\", \"Chinois\", \"Pékin\", \"Taïwan\",\n",
        "    \"Russie\", \"Russe\", \"Moscou\", \"Kremlin\",\n",
        "    \"Ukraine\", \"Ukrainien\", \"Kiev\",\n",
        "    # --- Specific Topics ---\n",
        "    \"Migration\", \"Migrants\", \"Réfugiés\", \"Asile\", \"Immigration\", \"Immigrés\", \"Flux migratoires\", \"Frontière\",\n",
        "    \"Guerre commerciale\", \"Commerce international\", \"Droit de douane\", \"Accords commerciaux\", \"Protectionnisme\", \"Multilateralisme\",\n",
        "    \"Nucléaire\", \"Prolifération\", \"AIEA\",\n",
        "    \"Énergie\", \"Pétrole\", \"Gaz\", \"OPEP\",\n",
        "    # --- Groups & Specific Entities ---\n",
        "    \"Hamas\", \"Jihad islamique\",\n",
        "    \"Hezbollah\",\n",
        "    \"Houthis\", \"Ansar Allah\",\n",
        "    \"Talibans\", \"Afghanistan\",\n",
        "    \"État islamique\", \"Daech\", \"EI\", \"ISIS\",\n",
        "    \"Al-Qaïda\",\n",
        "    # --- Political Ideologies ---\n",
        "    \"Extrême droite\", \"Populisme\",\n",
        "    \"Autoritarisme\", \"Souverainisme\", \"Nationalisme\", \"Islamisme\"\n",
        "]\n",
        "# === END PASTE keywords list ===\n",
        "print(f\"Loaded {len(political_keywords)} political keywords.\")\n",
        "\n",
        "# === ACTION REQUIRED: Paste your 'min_keyword_hits' assignment below ===\n",
        "# (Copied from Cell 12 of Perfect_results_Rafael_WithReport.ipynb)\n",
        "min_keyword_hits = 3\n",
        "# === END PASTE min_keyword_hits ===\n",
        "print(f\"Minimum keyword hits set to: {min_keyword_hits}\")\n",
        "\n",
        "\n",
        "# --- FUNCTION DEFINITIONS ---\n",
        "\n",
        "# 1. PDF Extraction Function\n",
        "# === ACTION REQUIRED: Paste your 'extract_text_from_pdf' function code below ===\n",
        "# (Copied from Cell 10 of Perfect_results_Rafael_WithReport.ipynb)\n",
        "# (Make sure it takes pdf_path as input for Colab testing)\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts all the text from a PDF file using pdfplumber.\"\"\"\n",
        "    full_text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    full_text += text + \"\\\\n\"\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"PDF file not found: {pdf_path}\")\n",
        "        print(f\"------ Error: PDF file not found at {pdf_path}\")\n",
        "        return None\n",
        "    except pdfplumber.PDFSyntaxError:\n",
        "        logging.error(f\"PDF syntax error in file: {pdf_path}\")\n",
        "        print(f\"------ Error: PDF syntax error in file: {pdf_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF: {e}\\\\n{traceback.format_exc()}\")\n",
        "        print(f\"------ Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "    print(f\"--- Extracted text from PDF: {os.path.basename(pdf_path)}\")\n",
        "    return full_text\n",
        "# === END PASTE extract_text_from_pdf ===\n",
        "\n",
        "\n",
        "# 2. Article Splitting Function (Split by 'ftp\\\\n' pattern)\n",
        "def smart_split_articles(full_text):\n",
        "    \"\"\"Splits text into potential articles based on the 'ftp\\\\n' separator\n",
        "       and applies basic cleaning and filtering.\"\"\"\n",
        "    if not full_text: return []\n",
        "    print(\"--- Running smart_split_articles (Splitting by 'ftp\\\\n') ---\")\n",
        "\n",
        "    # Basic cleaning (remove form feed, maybe consolidate multiple spaces later if needed)\n",
        "    cleaned_text = full_text.replace('\\\\x0c', '\\\\n')\n",
        "\n",
        "    # Split the text wherever 'ftp\\n' occurs\n",
        "    # This pattern likely separates major sections or pages\n",
        "    split_pattern = r'ftp\\\\n'\n",
        "    possible_articles = re.split(split_pattern, cleaned_text, flags=re.IGNORECASE) # Use re.split\n",
        "    print(f\"Number of chunks after splitting by '{split_pattern}': {len(possible_articles)}\")\n",
        "\n",
        "    # Filter out empty strings and apply length/content checks\n",
        "    final_articles = []\n",
        "    min_article_length = 150 # Minimum characters\n",
        "    print(f\"Filtering chunks shorter than {min_article_length} characters...\")\n",
        "    for i, article_chunk in enumerate(possible_articles):\n",
        "        if article_chunk:\n",
        "            # Further clean each chunk: remove leading/trailing whitespace\n",
        "            # and potentially remove lines that look like page headers/footers if needed\n",
        "            trimmed_chunk = article_chunk.strip()\n",
        "\n",
        "            # Remove potential leftover page/section headers at the start of a chunk\n",
        "            # Example: remove lines like 'VI VOTRE FAIT DU JOUR' or 'POLITIQUE 11' if they appear alone at the start\n",
        "            lines = trimmed_chunk.split('\\\\n')\n",
        "            if lines and re.match(r'^[A-Z\\s]+\\s*\\d*$', lines[0].strip()): # Check if first line looks like a header\n",
        "                 trimmed_chunk = '\\\\n'.join(lines[1:]).strip()\n",
        "\n",
        "            # Check length and if it contains at least some letters\n",
        "            if len(trimmed_chunk) > min_article_length and any(c.isalpha() for c in trimmed_chunk):\n",
        "                 final_articles.append(trimmed_chunk)\n",
        "            # else:\n",
        "                 # print(f\"  Chunk {i} filtered out (length {len(trimmed_chunk)} <= {min_article_length} or no letters/only header).\")\n",
        "\n",
        "\n",
        "    print(f\"  (Splitting resulted in {len(final_articles)} potential articles after filtering)\")\n",
        "    print(\"--- Finished smart_split_articles ---\")\n",
        "    return final_articles\n",
        "\n",
        "# 3. Summarization Function (Improved with Input Truncation)\n",
        "async def async_summarize_article(\n",
        "    aclient: AsyncOpenAI,\n",
        "    text: str,\n",
        "    max_lines: int = 4,\n",
        "    style: str = \"journalistique radio\",\n",
        "    tone: str = \"neutre et informatif\",\n",
        "    focus: str = \"faits politiques\",\n",
        "    # Add a safety limit slightly below the typical model max context\n",
        "    # (e.g., gpt-3.5-turbo often has 16k token limit, roughly 3-4 chars/token)\n",
        "    max_input_chars: int = 12000 # You can adjust this limit if needed\n",
        ") -> str:\n",
        "    \"\"\"Asynchronously summarizes an article using the provided AsyncOpenAI client,\n",
        "       truncating input text if it's too long.\"\"\"\n",
        "\n",
        "    # Basic check for valid input text\n",
        "    if not text or not isinstance(text, str) or len(text.strip()) < 50:\n",
        "        logging.warning(f\"Skipping summary for short/invalid text: {text[:50]}...\")\n",
        "        return \"Résumé non disponible (Texte d'entrée invalide).\"\n",
        "\n",
        "    # --- ADDED: Input Truncation ---\n",
        "    text_to_summarize = text # Start with original text\n",
        "    if len(text) > max_input_chars:\n",
        "        logging.warning(f\"Input text length ({len(text)}) exceeds limit ({max_input_chars}). Truncating.\")\n",
        "        print(f\"⚠️ Input text too long ({len(text)} chars), truncating to {max_input_chars} chars for summary.\")\n",
        "        text_to_summarize = text[:max_input_chars] # Use only the first part\n",
        "    # --- End Added ---\n",
        "\n",
        "    # Use the potentially truncated text in the prompt\n",
        "    prompt = f\"\"\"\n",
        "Résume cet article en {max_lines} lignes maximum, dans un style {style},\n",
        "en mettant l'accent sur les {focus}. Utilise un ton {tone}.\n",
        "\n",
        "Texte de l'article :\n",
        "{text_to_summarize}\n",
        "\"\"\"\n",
        "    print(f\"--> Preparing async summary request for article snippet: {text_to_summarize[:50]}...\")\n",
        "    if not aclient: return \"Résumé non disponible (Erreur Client OpenAI).\"\n",
        "    try:\n",
        "        response = await aclient.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\", # Ensure this model supports the context length\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=200 # Limit the output summary length too\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        print(f\"--> Received async summary for article snippet: {text_to_summarize[:50]}...\")\n",
        "        # Add note if text was truncated\n",
        "        if len(text) > max_input_chars:\n",
        "             summary += \"\\\\n*(Note: Résumé basé sur le début de l'article car le texte original était trop long.)*\"\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        # Log the specific error, including context length errors\n",
        "        logging.error(f\"Error summarizing article asynchronously: {e}\\\\nText Snippet: {text_to_summarize[:200]}\")\n",
        "        # Check if it's a context length error specifically\n",
        "        # The error object structure might vary, check common attributes\n",
        "        error_code = getattr(e, 'code', None) or getattr(getattr(e, 'body', {}), 'code', None)\n",
        "        if error_code == 'context_length_exceeded':\n",
        "            print(f\"----- Error during async summary: Input text still too long for the model even after truncation attempt.\")\n",
        "            return \"Résumé non disponible (Erreur: Texte trop long).\"\n",
        "        else:\n",
        "            print(f\"----- Error during async summary request for article snippet: {text_to_summarize[:50]}... Error: {e}\")\n",
        "            return \"Résumé non disponible (Erreur API).\"\n",
        "\n",
        "# 4. Translation Function\n",
        "# === ACTION REQUIRED: Paste your 'async_translate_to_arabic' function code below ===\n",
        "# (Copied from Cell 8 of Perfect_results_Rafael_WithReport.ipynb)\n",
        "async def async_translate_to_arabic(\n",
        "    aclient: AsyncOpenAI,\n",
        "    text: str,\n",
        "    style: str = \"journalistique\",\n",
        "    formality: str = \"formel\",\n",
        "    target_audience: str = \"public général\",\n",
        "    context: str = \"actualités\"\n",
        ") -> str:\n",
        "    \"\"\"Asynchronously translates text into Modern Standard Arabic.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Traduis ce texte en arabe standard moderne (MSA),\n",
        "avec une précision élevée et un style {style},\n",
        "adapté à un public {target_audience} dans un contexte de {context}.\n",
        "Utilise un registre {formality}.\n",
        "Conserve la structure et le sens du texte original.\n",
        "\n",
        "Texte à traduire :\n",
        "{text}\n",
        "\"\"\" # Ensure no characters after the closing triple quotes\n",
        "    print(f\"--> Preparing async translation request for text snippet: {text[:50]}...\")\n",
        "    if not aclient: return \"Traduction non disponible (Erreur Client OpenAI).\" # Added client check\n",
        "    # Handling the case where the input text might be the error message from summarization\n",
        "    if text == \"Résumé non disponible.\" or \"Erreur API\" in text or \"Résumé Non Généré/Vide\" in text:\n",
        "         print(f\"--> Skipping translation because input text indicates summary error: '{text[:50]}...'\")\n",
        "         return \"Traduction non disponible car le résumé initial n'était pas disponible ou contenait une erreur.\"\n",
        "    try:\n",
        "        response = await aclient.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "        )\n",
        "        translation = response.choices[0].message.content.strip()\n",
        "        print(f\"--> Received async translation for text snippet: {text[:50]}...\")\n",
        "        return translation\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error translating text asynchronously: {e}\\\\n{traceback.format_exc()}\\\\nText: {text[:200]}\")\n",
        "        print(f\"------ Error during async translation request for text snippet: {text[:50]}... Error: {e}\")\n",
        "        return \"Traduction non disponible (Erreur API).\"\n",
        "# === END PASTE async_translate_to_arabic ===\n",
        "\n",
        "\n",
        "# 5. Classification Function (using the fine-tuned model loaded in Cell 3)\n",
        "# === ACTION REQUIRED: Paste the 'classify_topic_finetuned' function definition below ===\n",
        "# (Copied from the previous response's app.py code)\n",
        "def classify_topic_finetuned(pipeline_obj, text_to_classify):\n",
        "    \"\"\"Classifies text using the fine-tuned model pipeline.\"\"\"\n",
        "    # Check if the pipeline object (loaded in Cell 3) exists\n",
        "    if not pipeline_obj:\n",
        "        print(\"----- Fine-tuned classifier not loaded/available. Skipping classification.\")\n",
        "        # Return a dictionary indicating the classifier is missing\n",
        "        return {\"label\": \"Classifier Unavailable\", \"score\": 0.0}\n",
        "\n",
        "    # Check if the text input is valid (not empty, is a string, has some length)\n",
        "    if not text_to_classify or not isinstance(text_to_classify, str) or len(text_to_classify.strip()) < 20:\n",
        "        print(f\"(!) Skipping fine-tuned classification for invalid/short input text: '{str(text_to_classify)[:50]}...'\")\n",
        "        # Return a dictionary indicating bad input\n",
        "        return {\"label\": \"Input Too Short/Invalid\", \"score\": 0.0}\n",
        "\n",
        "    try:\n",
        "        # Print a message showing the start of classification for this text\n",
        "        print(f\"  (Classifying with fine-tuned model: {text_to_classify[:50]}...)\")\n",
        "\n",
        "        # Run the pipeline! Give it the text (limit length for stability)\n",
        "        # The pipeline handles tokenization and prediction internally\n",
        "        result = pipeline_obj(text_to_classify[:512]) # Apply length limit (e.g., 512 tokens)\n",
        "\n",
        "        # The pipeline returns a list (usually with one item for single input)\n",
        "        # Each item is a dictionary like [{'label': 'politics', 'score': 0.9...}]\n",
        "        if result and isinstance(result, list):\n",
        "            top_prediction = result[0] # Get the first (and likely only) prediction dictionary\n",
        "            label = top_prediction.get('label', 'Error') # Get the predicted label name\n",
        "            score = top_prediction.get('score', 0.0)   # Get the confidence score\n",
        "            print(f\"  (Fine-tuned classification: Label='{label}', Score={score:.4f})\")\n",
        "            # Return the result in a consistent dictionary format\n",
        "            return {\"label\": label, \"score\": score}\n",
        "        else:\n",
        "             # Handle unexpected output format from the pipeline\n",
        "             print(f\"----- Unexpected result format from fine-tuned pipeline: {result}\")\n",
        "             return {\"label\": \"Classification Error (Format)\", \"score\": 0.0}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any error during the classification process\n",
        "        print(f\"----- Error during fine-tuned classification: {e}\")\n",
        "        print(traceback.format_exc()) # Print detailed error stack\n",
        "        # Return a dictionary indicating a runtime error\n",
        "        return {\"label\": \"Classification Error (Runtime)\", \"score\": 0.0}\n",
        "# === END PASTE classify_topic_finetuned ===\n",
        "\n",
        "\n",
        "print(\"\\n --- Helper functions and variables defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZbO0U_DR4Ls",
        "outputId": "491b9d8a-42a0-4b56-9744-126717fa0343"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining helper functions and variables...\n",
            "Loaded 182 political keywords.\n",
            "Minimum keyword hits set to: 3\n",
            "\n",
            " --- Helper functions and variables defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Helper Functions & Variables\n",
        "\n",
        "Explanation: This is a crucial cell where you bring together all the individual processing steps. You need to copy the exact Python code for your helper functions from the Perfect_results_Rafael_WithReport.ipynb notebook. This includes extracting text from PDFs, splitting text into articles, summarizing articles (using OpenAI), translating summaries (using OpenAI), and the new function we define here to use your fine-tuned model for classification. We also copy the political_keywords and min_keyword_hits variables."
      ],
      "metadata": {
        "id": "OhwCKD1QSDeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test the Combined Logic\n",
        "\n",
        "# Import necessary libraries (some might be redundant)\n",
        "import asyncio\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "# --- Define Input Source ---\n",
        "# Change this to \"pdf\" to test PDF processing\n",
        "input_source = \"pdf\"  # Options: \"paste\" or \"pdf\"\n",
        "\n",
        "# --- Define Test Input ---\n",
        "# Option 1: Pasted Text (if input_source = \"paste\")\n",
        "test_pasted_text = \"\"\"\n",
        "Philippe-Attal, la bataille qui vient dans le camp présidentiel  Tout en se ménageant, les deux ex-premiers ministres tiennent à se différencier, sans primaire, pour que l’un d’eux s’impose comme candidat unique du « bloc central ».   10 min • Louis Hausalter Loris Boichot Tristan Quinault-Maupoil É douard Philippe et Gabriel Attal se sont donné rendez-vous chez un spécialiste des rouleaux de printemps et des raviolis de crevettes. Ce 19 février, les deux anciens premiers ministres se retrouvent autour de la table de Lily Wang, un chic restaurant asiatique du 7e arrondissement de Paris. Le patron du parti présidentiel, Renaissance, invite son homologue d’Horizons à son rassemblement militant du 6 avril, prévu à Saint-Denis (Seine-Saint-Denis). Là où il prévoit d’assumer un « premier pas » vers l’élection présidentielle de 2027, à son tour. Son interlocuteur, pour sa part déjà officiellement candidat, s’est lancé dans une série de meetings régionaux.  Les temps sont à la préparation de l’après-Macron, sous la surveillance de figures du « bloc central » méfiantes devant ce duel d’ambitieux anciens premiers ministres. « Avant de se demander qui porte un projet, il faudrait qu’il y ait un projet, pointe la ministre de l’Éducation nationale, Élisabeth Borne, face à ce qu’elle appelle la « bande des garçons ». Il faudra envoyer des messages de ce qu’on veut faire, et non pas juste dire : “Je suis là.” » C’est maintenant en dehors du pouvoir exécutif, tournés vers l’avenir, que s’organisent l’ex-Républicain (LR) et l’ancien socialiste, tous deux devenus personnalités politiques préférées des Français à leur sortie du gouvernement. Pour la présidentielle, « ce sont les deux plus attendus, les plus crédibles dans le bloc central », remarque un cadre de Renaissance. Et d’ajouter, beau joueur : « Quoi qu’il arrive, le futur du pays se fera avec Édouard Philippe et Gabriel Attal. »  Entre le maire du Havre (Seine-Maritime), 54 ans, et son rival, de dix-huit ans son cadet, il y aurait de la cordialité, « sans tensions », selon l’entourage du premier, ni « guerre larvée » ou « animosité », selon les proches du second. Tous deux membres du même gouvernement de 2018 à 2020, ils partagent des orientations semblables - proeuropéennes, libérales en économie et fermes en matière régalienne. Mais les divergences ne manquent pas. La polémique récente sur le port du voile lors des compétitions sportives en a apporté l’illustration. Le président d’Horizons a exprimé ses réticences devant la proposition de loi de LR visant à l’interdire ; Gabriel Attal en a profité pour s’y engouffrer et marquer sa différence. Une manière de rassurer sa droite, alors qu’il cherche à apparaître comme le garant du « dépassement » des clivages gauche-droite, d’une forme de « en même temps », davantage remis en cause par Édouard Philippe, autoproclamé « homme de droite ». À l’égard de François Bayrou, le trentenaire a fait le choix de la bienveillance, alors qu’Édouard Philippe répète qu’aucune réforme majeure ne pourra voir le jour d’ici la prochaine présidentielle.\n",
        "\"\"\" # Add more text if needed\n",
        "\n",
        "# Option 2: PDF Path (if input_source = \"pdf\")\n",
        "# ** IMPORTANT: Make sure this PDF exists on your Google Drive! **\n",
        "test_pdf_path = \"/content/drive/MyDrive/RadioBrief/Le Parisien du Mercredi 09 Avril 2025.pdf\"\n",
        "\n",
        "# --- Main Test Logic (Async Function) ---\n",
        "async def run_test():\n",
        "    print(\"\\\\n--- Starting Logic Test ---\")\n",
        "    full_text = None\n",
        "    source_name = \"\"\n",
        "    processing_error = False\n",
        "\n",
        "    # 1. Get Input Text\n",
        "    print(f\"Input source selected: {input_source}\")\n",
        "    if input_source == \"pdf\":\n",
        "        if os.path.exists(test_pdf_path):\n",
        "            print(f\"Extracting text from PDF: {test_pdf_path}\")\n",
        "            full_text = extract_text_from_pdf(test_pdf_path) # Use function from Cell 4\n",
        "            source_name = os.path.basename(test_pdf_path)\n",
        "            if full_text is None:\n",
        "                print(\"----- PDF extraction failed.\")\n",
        "                processing_error = True\n",
        "        else:\n",
        "            print(f\"----- Test PDF not found at: {test_pdf_path}\")\n",
        "            processing_error = True\n",
        "    elif input_source == \"paste\":\n",
        "        if test_pasted_text and test_pasted_text.strip():\n",
        "            full_text = test_pasted_text\n",
        "            source_name = \"Pasted Text\"\n",
        "            print(\"Using pasted text.\")\n",
        "        else:\n",
        "            print(\"----- Test pasted text is empty.\")\n",
        "            processing_error = True\n",
        "    else:\n",
        "        print(\"----- Invalid input_source selected.\")\n",
        "        processing_error = True\n",
        "\n",
        "    # Stop if input failed\n",
        "    if processing_error or not full_text:\n",
        "        print(\"--- Test Aborted due to input error ---\")\n",
        "        return\n",
        "\n",
        "    # 2. Split Articles\n",
        "    print(f\"\\\\nProcessing content from: {source_name}\")\n",
        "    # Ensure smart_split_articles is defined (should be in Cell 4)\n",
        "    if 'smart_split_articles' not in globals():\n",
        "        print(\"----- Error: smart_split_articles function not defined.\")\n",
        "        return\n",
        "    articles = smart_split_articles(full_text)\n",
        "\n",
        "    if not articles:\n",
        "        print(\"----- Could not split the text into articles using the defined rules.\")\n",
        "        processing_error = True\n",
        "    else:\n",
        "        print(f\"Found {len(articles)} potential articles. Identifying the first political one...\")\n",
        "\n",
        "        first_political_article = None\n",
        "        first_political_topic = \"Non Politique / Non Trouvé\"\n",
        "        predicted_topic_label = \"N/A\"\n",
        "        predicted_topic_score = 0.0\n",
        "\n",
        "        # Compile keyword regex (ensure 're' and 'political_keywords' are available)\n",
        "        if 're' not in globals() or 'political_keywords' not in globals():\n",
        "             print(\"----- Error: 're' module or 'political_keywords' not defined.\")\n",
        "             return\n",
        "        keyword_pattern = r'(?i)\\\\b(?:' + '|'.join(re.escape(kw) for kw in political_keywords) + r')\\\\b'\n",
        "        keyword_regex = re.compile(keyword_pattern)\n",
        "\n",
        "        # Define which MasakhaNEWS labels count as \"political\" for filtering\n",
        "        # ** Adjust this list based on your needs and the model's labels **\n",
        "        relevant_masakha_labels = ['politics']\n",
        "        # relevant_masakha_labels = ['politics', 'business']\n",
        "\n",
        "        print(f\"Considering these fine-tuned labels as 'political': {relevant_masakha_labels}\")\n",
        "\n",
        "        # --- 3. Find and Process First Political Article (IMPROVED LOGIC) ---\n",
        "        first_political_article_found = False\n",
        "        # Compile keyword regex once\n",
        "        if 're' not in globals() or 'political_keywords' not in globals() or 'min_keyword_hits' not in globals():\n",
        "             print(\"----- Error: 're' module, 'political_keywords', or 'min_keyword_hits' not defined.\"); return\n",
        "        keyword_pattern = r'(?i)\\\\b(?:' + '|'.join(re.escape(kw) for kw in political_keywords) + r')\\\\b'\n",
        "        keyword_regex = re.compile(keyword_pattern)\n",
        "        # relevant_masakha_labels = ['politics', 'business']\n",
        "        relevant_masakha_labels = ['politics'] # Labels considered \"political\"\n",
        "        print(f\"Considering these fine-tuned labels as 'political': {relevant_masakha_labels}\")\n",
        "\n",
        "        # Loop through the *filtered* articles from smart_split_articles\n",
        "        for i, article_text in enumerate(articles):\n",
        "            # Note: article_text here is already stripped and length-filtered by smart_split_articles\n",
        "            print(f\"\\n--- Checking Article Chunk {i+1} ---\")\n",
        "            print(f\"Snippet: {article_text[:100]}...\") # Show snippet of current article\n",
        "\n",
        "            is_political = False\n",
        "            assigned_topic_source = \"N/A\"\n",
        "            predicted_topic_label = \"N/A\"\n",
        "            predicted_topic_score = 0.0\n",
        "\n",
        "            # --- Classification ---\n",
        "            if 'classifier_finetuned' in globals() and classifier_finetuned and 'classify_topic_finetuned' in globals():\n",
        "                classification_result = classify_topic_finetuned(classifier_finetuned, article_text) # Classify THIS article\n",
        "                predicted_topic_label = classification_result.get('label', 'Error')\n",
        "                predicted_topic_score = classification_result.get('score', 0.0)\n",
        "                if predicted_topic_label in relevant_masakha_labels:\n",
        "                    is_political = True\n",
        "                    assigned_topic_source = f\"Classifier ({predicted_topic_label})\"\n",
        "                    print(f\"  -> Identified as '{predicted_topic_label}' by classifier (Score: {predicted_topic_score:.2f}).\")\n",
        "            else:\n",
        "                predicted_topic_label = \"Classifier Unavailable\"\n",
        "                print(\"  -> Skipping classification (model/function missing). Checking keywords...\")\n",
        "\n",
        "            # --- Keyword Check ---\n",
        "            keyword_matches = keyword_regex.findall(article_text)\n",
        "            number_of_hits = len(keyword_matches)\n",
        "            if number_of_hits >= min_keyword_hits:\n",
        "                print(f\"  -> Met keyword threshold ({number_of_hits} hits).\")\n",
        "                if not is_political: # Mark as political if not already done by classifier\n",
        "                    is_political = True\n",
        "                    assigned_topic_source = f\"Keywords ({number_of_hits} hits)\"\n",
        "\n",
        "            # --- Process if Political ---\n",
        "            if is_political:\n",
        "                print(f\"✅ Political article found (Index {i+1}) identified via {assigned_topic_source}.\")\n",
        "                print(\"\\\\n--- Processing This Article ---\")\n",
        "\n",
        "                # Check OpenAI client\n",
        "                if 'async_client' not in globals() or not async_client:\n",
        "                     print(\"----- OpenAI client not initialized, cannot summarize/translate.\")\n",
        "                     break # Stop processing if client fails\n",
        "\n",
        "                # Check functions exist\n",
        "                if 'async_summarize_article' not in globals() or 'async_translate_to_arabic' not in globals():\n",
        "                     print(\"----- Error: Summarization or translation function not defined.\")\n",
        "                     break\n",
        "\n",
        "                try:\n",
        "                    print(\"Running Summary and Translation...\")\n",
        "                    # Pass THIS specific article_text to the functions\n",
        "                    summary_task = asyncio.create_task(async_summarize_article(async_client, article_text))\n",
        "                    summary_result = await summary_task\n",
        "\n",
        "                    translation_result = \"Translation skipped (summary error).\"\n",
        "                    if summary_result and \\\n",
        "                       \"Erreur API\" not in summary_result and \\\n",
        "                       \"non disponible\" not in summary_result and \\\n",
        "                       \"Texte d'entrée invalide\" not in summary_result and \\\n",
        "                       \"Texte trop long\" not in summary_result:\n",
        "                        translation_task = asyncio.create_task(async_translate_to_arabic(async_client, summary_result))\n",
        "                        translation_result = await translation_task\n",
        "                    else:\n",
        "                        translation_result = f\"Traduction non effectuée ({summary_result}).\"\n",
        "\n",
        "                    # --- Display Results for THIS article ---\n",
        "                    print(\"\\\\n--- FINAL TEST RESULTS (for first political article found) ---\")\n",
        "                    print(f\"----- Detected Topic (Fine-Tuned Model): {predicted_topic_label} (Score: {predicted_topic_score:.2f})\") # Use the label found for this article\n",
        "                    print(f\"\\\\n----- Summary (French):\\\\n{summary_result}\")\n",
        "                    print(f\"\\\\n----- Translation (Arabic):\\\\n{translation_result}\")\n",
        "                    print(f\"\\\\n----- Original Snippet (first 500 chars):\\\\n{article_text[:500]}...\")\n",
        "\n",
        "                    first_political_article_found = True # Mark that we found and processed one\n",
        "                    break # IMPORTANT: Stop after processing the first political article\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"----- An error occurred during API processing for article {i+1}: {e}\")\n",
        "                    print(traceback.format_exc())\n",
        "                    # Optionally decide if you want to break or continue to next article on API error\n",
        "                    break # Stop for now if API fails\n",
        "\n",
        "        # --- After the loop ---\n",
        "        if not first_political_article_found and not processing_error:\n",
        "            print(\"\\\\n----- No political articles meeting the criteria were found in the input after checking all chunks.\")\n",
        "\n",
        "    print(\"\\\\n--- Logic Test Finished ---\")\n",
        "\n",
        "# --- Run the Test ---\n",
        "# Using asyncio.run() to execute the async test function\n",
        "# Check necessary components exist before running\n",
        "if __name__ == '__main__' and \\\n",
        "   'async_client' in globals() and async_client and \\\n",
        "   'classifier_finetuned' in globals(): # Check classifier too, even if it might be None\n",
        "     try:\n",
        "         # nest_asyncio allows running within an existing loop (like Colab's)\n",
        "         print(\"\\nAttempting to run the async test logic...\")\n",
        "         asyncio.run(run_test())\n",
        "         print(\"Async test logic execution complete.\")\n",
        "     except Exception as e:\n",
        "         print(f\"----- An error occurred trying to run the async test: {e}\")\n",
        "         print(traceback.format_exc())\n",
        "else:\n",
        "    print(\"\\\\nSkipping test run because OpenAI client ('async_client') or Classifier ('classifier_finetuned') is not available or not loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucULx5M_SPGs",
        "outputId": "cdff8233-bddb-4180-c85a-3bc8e627b147"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting to run the async test logic...\n",
            "\\n--- Starting Logic Test ---\n",
            "Input source selected: pdf\n",
            "Extracting text from PDF: /content/drive/MyDrive/RadioBrief/Le Parisien du Mercredi 09 Avril 2025.pdf\n",
            "--- Extracted text from PDF: Le Parisien du Mercredi 09 Avril 2025.pdf\n",
            "\\nProcessing content from: Le Parisien du Mercredi 09 Avril 2025.pdf\n",
            "--- Running smart_split_articles (Splitting by 'ftp\\n') ---\n",
            "Number of chunks after splitting by 'ftp\\\\n': 45\n",
            "Filtering chunks shorter than 150 characters...\n",
            "  (Splitting resulted in 44 potential articles after filtering)\n",
            "--- Finished smart_split_articles ---\n",
            "Found 44 potential articles. Identifying the first political one...\n",
            "Considering these fine-tuned labels as 'political': ['politics']\n",
            "Considering these fine-tuned labels as 'political': ['politics']\n",
            "\n",
            "--- Checking Article Chunk 1 ---\n",
            "Snippet: 75\n",
            "Un incendie Gare aux fermetures cet été\n",
            "Centre de tri à Paris RER C\n",
            "sans danger pour la santé ? a...\n",
            "  (Classifying with fine-tuned model: 75\n",
            "Un incendie Gare aux fermetures cet été\n",
            "Centre ...)\n",
            "  (Fine-tuned classification: Label='business', Score=0.8662)\n",
            "\n",
            "--- Checking Article Chunk 2 ---\n",
            "Snippet: 2 FAIT DU JOUR LIGUE DES CHAMPIONS\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "L'édito\n",
            "Frédéric\n",
            "Michel\n",
            "Rédacteur ...\n",
            "  (Classifying with fine-tuned model: 2 FAIT DU JOUR LIGUE DES CHAMPIONS\n",
            "Mercredi 9 avri...)\n",
            "  (Fine-tuned classification: Label='sports', Score=0.9401)\n",
            "\n",
            "--- Checking Article Chunk 3 ---\n",
            "Snippet: 3\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "FANS I\n",
            "Le prince « Il n’y a pas qu’un seul\n",
            "William\n",
            "joueur qui porte...\n",
            "  (Classifying with fine-tuned model: 3\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "FANS I\n",
            "Le prince ...)\n",
            "  (Fine-tuned classification: Label='sports', Score=0.9401)\n",
            "\n",
            "--- Checking Article Chunk 4 ---\n",
            "Snippet: 4 FAIT DU JOUR LIGUE DES CHAMPIONS\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "Même pas peur !\n",
            "QUARTS DE FINALE A...\n",
            "  (Classifying with fine-tuned model: 4 FAIT DU JOUR LIGUE DES CHAMPIONS\n",
            "Mercredi 9 avri...)\n",
            "  (Fine-tuned classification: Label='sports', Score=0.9404)\n",
            "\n",
            "--- Checking Article Chunk 5 ---\n",
            "Snippet: 5\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "GARDIENS I\n",
            "Donnarumma - Martinez, duel de géants\n",
            "Benjamin Quarez dé...\n",
            "  (Classifying with fine-tuned model: 5\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "GARDIENS I\n",
            "Donnar...)\n",
            "  (Fine-tuned classification: Label='sports', Score=0.9397)\n",
            "\n",
            "--- Checking Article Chunk 6 ---\n",
            "Snippet: 6 SPÉCIAL DROITS DE DOUANE\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "En réponse à l’entrée en vigueur,\n",
            "dès ce m...\n",
            "  (Classifying with fine-tuned model: 6 SPÉCIAL DROITS DE DOUANE\n",
            "Mercredi 9 avril 2025NN...)\n",
            "  (Fine-tuned classification: Label='business', Score=0.8846)\n",
            "\n",
            "--- Checking Article Chunk 7 ---\n",
            "Snippet: 7\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "RIPOSTE I\n",
            "Quels leviers\n",
            "pour l’Europe ?\n",
            "C’EST LA RELATION com- une ...\n",
            "  (Classifying with fine-tuned model: 7\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "RIPOSTE I\n",
            "Quels l...)\n",
            "  (Fine-tuned classification: Label='business', Score=0.8842)\n",
            "\n",
            "--- Checking Article Chunk 8 ---\n",
            "Snippet: 8 SPÉCIAL DROITS DE DOUANE\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "La grande\n",
            "confrontation\n",
            "est bien lancée\n",
            "Pé...\n",
            "  (Classifying with fine-tuned model: 8 SPÉCIAL DROITS DE DOUANE\n",
            "Mercredi 9 avril 2025NN...)\n",
            "  (Fine-tuned classification: Label='business', Score=0.8688)\n",
            "\n",
            "--- Checking Article Chunk 9 ---\n",
            "Snippet: 9\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "RUOFUD\n",
            "DERF/PFA\n",
            "Donald Trump (à g.) L’amertume des fans\n",
            "et Xi Jinpi...\n",
            "  (Classifying with fine-tuned model: 9\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "RUOFUD\n",
            "DERF/PFA\n",
            "D...)\n",
            "  (Fine-tuned classification: Label='business', Score=0.8840)\n",
            "\n",
            "--- Checking Article Chunk 10 ---\n",
            "Snippet: 10 INTERNATIONAL\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "situation n’a été aussi grave et garantie de la sécu...\n",
            "  (Classifying with fine-tuned model: 10 INTERNATIONAL\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "si...)\n",
            "  (Fine-tuned classification: Label='politics', Score=0.8830)\n",
            "  -> Identified as 'politics' by classifier (Score: 0.88).\n",
            "✅ Political article found (Index 10) identified via Classifier (politics).\n",
            "\\n--- Processing This Article ---\n",
            "Running Summary and Translation...\n",
            "--> Preparing async summary request for article snippet: 10 INTERNATIONAL\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "si...\n",
            "--> Received async summary for article snippet: 10 INTERNATIONAL\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "si...\n",
            "--> Preparing async translation request for text snippet: Le président Macron alerte sur la situation intena...\n",
            "--> Received async translation for text snippet: Le président Macron alerte sur la situation intena...\n",
            "\\n--- FINAL TEST RESULTS (for first political article found) ---\n",
            "📌 Detected Topic (Fine-Tuned Model): politics (Score: 0.88)\n",
            "\\n📝 Summary (French):\\nLe président Macron alerte sur la situation intenable à Gaza lors d'une visite en Égypte. Il plaide pour une reprise de l'aide humanitaire et un processus politique pour résoudre la crise. Il met en garde contre une possible pénurie d'eau potable et critique la politique américaine. Macron s'engage à continuer ses efforts diplomatiques pour trouver une solution au conflit.\n",
            "\\n🌍 Translation (Arabic):\\nحذر الرئيس ماكرون من الوضع غير المحتمل في غزة خلال زيارته إلى مصر. وناشد بإعادة تقديم المساعدات الإنسانية وبدء عملية سياسية لحل الأزمة. وحذر من احتمالية نقص المياه الصالحة للشرب وانتقد السياسة الأمريكية. وعازم ماكرون على مواصلة جهوده الدبلوماسية لإيجاد حل للصراع.\n",
            "\\n📰 Original Snippet (first 500 chars):\\n10 INTERNATIONAL\n",
            "Mercredi 9 avril 2025NN° 25077\n",
            "situation n’a été aussi grave et garantie de la sécurité des cient qu’une réouverture de la\n",
            "intenable. » De l’autre côté de bénévoles, puis un cessez-le- frontière entre l’Égypte et la\n",
            "la frontière, 1 300 Palestiniens feu pour quarante ou cinquan- Palestine à des fins exclusive-\n",
            "ont été tués depuis la fin du te jours avec libération d’ota- ment humanitaires ne per-\n",
            "cessez-le-feu le mois dernier, ges, enfin la reprise d’un mettrait toutefois pas de\n",
            "...\n",
            "\\n--- Logic Test Finished ---\n",
            "Async test logic execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Section\n",
        "\n",
        "Explanation: This final cell puts everything together. It simulates the core logic: getting text (either from a test PDF path on your Drive or from pasted text you define here), splitting it, finding the first political article (using the fine-tuned classifier OR keywords), and then summarizing/translating that article. The results are printed at the end.\n"
      ],
      "metadata": {
        "id": "2yPAvq9uSSoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report: Streamlit_App_Logic_Test NotebookThis report details the origins of the code used in the Streamlit_App_Logic_Test (1).ipynb notebook and analyzes the output from its Cell 5 execution using PDF input.\n",
        "\n",
        "Part 1: Code Component OriginsThe Streamlit_App_Logic_Test (1).ipynb notebook was constructed by combining and adapting components from two primary source notebooks:\n",
        "\n",
        "Perfect_results_Rafael_WithReport.ipynb (Main Processing Pipeline):Setup: Core library imports (pandas, pdfplumber, re, asyncio, openai, datetime, logging, traceback, os).Variables (Cell 4):political_keywords: Directly copied from Cell 6.min_keyword_hits: Directly copied from Cell 12.Functions (Cell 4):extract_text_from_pdf: Directly copied from Cell 10 (using the version taking a file path).smart_split_articles: Directly copied from Cell 11 (version without secondary ALL CAPS split), with debug print statements added for testing. (Note: This is the function currently causing issues).async_summarize_article: Based on Cell 7, but modified in Cell 4 of the test notebook to include input text truncation (max_input_chars check) to resolve the context_length_exceeded error encountered during testing.async_translate_to_arabic: Directly copied from Cell 8, with minor improvements to error message handling.Core Logic Structure (Cell 5): The overall flow (get text -> split -> loop/identify -> process -> display) is adapted from the logic in Cells 12 and 14, but modified to run as a single test sequence and incorporate the fine-tuned classifier.\n",
        "\n",
        "FineTune_Topic_Classifier (2).ipynb (Fine-Tuning Notebook):Model Loading (Cell 3):The logic to load the saved fine-tuned model (/content/drive/MyDrive/RadioBrief/finetune-results/final_model) using AutoModelForSequenceClassification.from_pretrained and AutoTokenizer.from_pretrained is taken directly from the inference/loading examples in the fine-tuning notebook (e.g., Cell 13).The id2label_map, label2id_map, and num_model_labels variables are copied from the data preparation stage (Cell 4) of the fine-tuning notebook to ensure compatibility with the loaded model.The creation of the pipeline(\"text-classification\", ...) object (classifier_finetuned) uses the loaded model and tokenizer.Classification Function (Cell 4):The classify_topic_finetuned function was newly written for the test notebook (Cell 4). It is designed specifically to use the classifier_finetuned pipeline loaded in Cell 3. It replaces the original classify_topic function from Perfect_results_Rafael_WithReport.ipynb (Cell 9), which used the different zero-shot model.\n",
        "\n",
        "Other Components:Library Installation (Cell 1): Includes all libraries needed by both source notebooks, plus streamlit and nest_asyncio.Drive Mount & API Key Loading (Cell 2): Standard Colab practices, adapted to read the key directly from openai_key.txt per user instruction.Debug Prints: Added throughout smart_split_articles and the main test logic in Cell 5 to aid troubleshooting.\n",
        "\n",
        "Conclusion on Code Origins: The test notebook accurately reflects the intended combination: core processing from the main notebook, classification using the fine-tuned model artifact, and necessary adaptations (like summary truncation) identified during testing.\n",
        "\n",
        "Part 2: Analysis of Cell 5 Output (PDF Input)The execution of Cell 5 with the PDF input yielded the following key results and indicated one primary remaining issue:\n",
        "\n",
        "Successful Steps:The notebook ran without Python errors.PDF text extraction was successful.The fine-tuned classification model (classifier_finetuned) loaded and made a prediction ('business') on the input it received.The input truncation logic added to async_summarize_article worked correctly, preventing the previous context_length_exceeded error from OpenAI by shortening the very long input text.Summarization and Translation API calls completed successfully on the truncated text.\n",
        "\n",
        "Core Problem: Article Segmentation FailureThe debug output from the smart_split_articles function clearly shows:Number of chunks after re.split: 1\n",
        "(Splitting attempt resulted in 1 potential articles after filtering)\n",
        "This confirms that the function, using the regular expression based on section keywords (FRANCE, INTERNATIONAL, etc.), failed to split the text extracted from this specific PDF. It treated the entire document (332,797 characters) as a single article chunk.\n",
        "\n",
        "Consequences:Because the text wasn't split, subsequent steps operated on the entire document instead of individual articles.The classification ('business') was performed on this large, mixed chunk, making the result likely inaccurate for any specific article within it.The summarization, while successful after truncation, only summarized the beginning of the document, not a targeted political article.The final displayed results (Topic: business, Summary: about PSG) are therefore misleading and do not reflect the successful processing of distinct political articles as intended.\n",
        "\n",
        "Diagnosis: The failure lies specifically within the smart_split_articles function (defined in Cell 4). The regular expression used (split_pattern_flexible) is not matching the section keywords as they appear in the text extracted by pdfplumber in this execution context. This could be due to subtle differences in formatting (spaces, newlines like \\n vs \\r\\n, hidden characters) compared to previous successful runs or the original notebook environment.\n",
        "\n",
        "Required Action: The immediate next step is to debug and refine the split_pattern_flexible regular expression within the smart_split_articles function in Cell 4. This requires examining the repr(cleaned_text[:1000]) debug output from Cell 5 to see the exact formatting of the extracted text and adjusting the regex pattern to correctly identify the section keywords as they appear in that specific text. Once the splitting function reliably produces multiple article chunks from the PDF, the rest of the pipeline should yield meaningful results.\n"
      ],
      "metadata": {
        "id": "45sABxkcoC0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Key Modifications for PDF Processing LogicTo achieve the final successful result where the PDF was correctly processed in the Streamlit_App_Logic_Test.ipynb notebook, several crucial modifications were made iteratively:\n",
        "\n",
        "Corrected API Key Loading: The initial method using .env failed. The code was modified to read the OpenAI API key directly from the specified openai_key.txt file, ensuring the OpenAI client could be initialized.\n",
        "\n",
        "Implemented Input Truncation for Summarization: The initial runs failed with an OpenAI context_length_exceeded error because the entire (unsplit) PDF text was sent for summarization.Fix: The async_summarize_article function was modified to include a character limit (max_input_chars). If the input text exceeded this limit, it was truncated before being sent to the API. This successfully resolved the API error, although it initially resulted in summaries of the document's beginning rather than a specific article.\n",
        "\n",
        "Revised Article Splitting Strategy: The core problem was that neither the original keyword-based regex nor splitting by double newlines (\\n\\n) effectively segmented the text extracted from the test PDF.\n",
        "Analysis: Examination of the fully extracted text revealed a consistent ftp\\n pattern, likely indicating page or major section breaks.Fix: The smart_split_articles function was entirely rewritten to use re.split(r'ftp\\\\n', ...) . This method successfully split the document into numerous (45) distinct chunks.\n",
        "\n",
        "Refined Article Selection Loop: After successful splitting, the logic still processed the incorrect (large, initial) chunk.Fix: The for loop in the main test logic (Cell 5) was adjusted to correctly iterate through the list of chunks produced by the new smart_split_articles function (potentially skipping the first chunk if deemed necessary) and, crucially, to pass the text of the specific chunk identified as political to the summarization and translation functions.\n",
        "\n",
        "Outcome: These combined modifications, particularly the change in splitting strategy (ftp\\n) and ensuring the correct article chunk text was processed, led to the final successful output where the PDF was correctly segmented, a relevant article chunk was identified and processed without context length errors, yielding a meaningful summary and translation."
      ],
      "metadata": {
        "id": "r5j6WCgqGjky"
      }
    }
  ]
}